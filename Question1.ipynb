{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe6a94df",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b889ae0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  HADM_ID                                         SHORT-TEXT  \\\n",
      "0           1   100003  history of present illness mr known lastname i...   \n",
      "1           3   100009  history of present illness yo man with known c...   \n",
      "2           5   100011  history of present illness y o male helmeted m...   \n",
      "3           9   100021  history of present illness year old spanish sp...   \n",
      "4          11   100028  history of present illness this is a year old ...   \n",
      "\n",
      "                                           ICD9_CODE  \\\n",
      "0  53100, 2851, 07054, 5715, 45621, 53789, 4019, ...   \n",
      "1  41401, 99604, 4142, 25000, 27800, V8535, 4148,...   \n",
      "2  85206, 82111, 86403, 48242, 8600, 2851, 86121,...   \n",
      "3  E8788, E8497, 4019, 04104, 0413, 5728, V1581, ...   \n",
      "4  5761, 0389, 5184, 57481, 99591, 42731, 2875, 8...   \n",
      "\n",
      "                                                ICD9    Label  \n",
      "0        531, 285, 070, 571, 456, 537, 401, 535, 782      285  \n",
      "1  414, 996, 414, 250, 278, V85, 414, 411, V45, V...      285  \n",
      "2  852, 821, 864, 482, 860, 285, 861, 807, 822, 8...      285  \n",
      "3  E87, E84, 401, 041, 041, 572, V15, 281, 780, 8...  281,287  \n",
      "4        576, 038, 518, 574, 995, 427, 287, 873, E92      287  \n",
      "   Unnamed: 0  HADM_ID                                               TEXT  \\\n",
      "0          34   100199  Admission Date:  [**2185-11-11**]       Discha...   \n",
      "1          46   100227  Admission Date:  [**2160-12-7**]              ...   \n",
      "2          88   100452  Admission Date:  [**2187-10-23**]             ...   \n",
      "3          89   100456  Admission Date:  [**2123-4-26**]              ...   \n",
      "4         107   100530  Admission Date:  [**2124-2-14**]              ...   \n",
      "\n",
      "   LABLE                                            entites  group  \n",
      "0    280  [\"Alzheimer's dementia\", 'coronary artery\\ndis...      1  \n",
      "1    280  ['Methotrexate', 'UTI', 'rheumatoid arthritis'...      1  \n",
      "2    280  ['Allergies', 'syncope', 'DVT', 'CHF', 'HTN', ...      1  \n",
      "3    280  ['Allergies', 'Codeine', 'Remicade', 'Vancomyc...      1  \n",
      "4    280  ['Penicillins', 'seizure', 'hematemesis', 'tra...      1  \n",
      "   HADM_ID                                               TEXT  \\\n",
      "0   100001  Admission Date:  [**2117-9-11**]              ...   \n",
      "1   100003  Admission Date:  [**2150-4-17**]              ...   \n",
      "2   100006  Admission Date:  [**2108-4-6**]       Discharg...   \n",
      "3   100007  Admission Date:  [**2145-3-31**]              ...   \n",
      "4   100009  Admission Date:  [**2162-5-16**]              ...   \n",
      "\n",
      "                                           ICD9_CODE  Label  \n",
      "0  250, 337, 584, 578, V58, 250, 536, 458, 250, 4...      0  \n",
      "1        531, 285, 070, 571, 456, 537, 401, 535, 782      1  \n",
      "2        493, 518, 486, 203, 276, 785, 309, V12, V15      0  \n",
      "3                            560, 557, 997, 486, 401      0  \n",
      "4  414, 996, 414, 250, 278, V85, 414, 411, V45, V...      1  \n",
      "   HADM_ID                                               TEXT LABLE\n",
      "0   100003  Admission Date:  [**2150-4-17**]              ...   285\n",
      "1   100009  Admission Date:  [**2162-5-16**]              ...   285\n",
      "2   100011  Admission Date:  [**2177-8-29**]              ...   285\n",
      "3   100021  Admission Date:  [**2109-8-17**]              ...   281\n",
      "4   100030  Admission Date:  [**2199-12-3**]       Dischar...   285\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('CSV1.csv')\n",
    "df2 = pd.read_csv('CSV2.csv')\n",
    "df3 = pd.read_csv('CSV3.csv')\n",
    "df4 = pd.read_csv('CSV4.csv')\n",
    "\n",
    "print(df1.head())\n",
    "print(df2.head())\n",
    "print(df3.head())\n",
    "print(df4.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011884da",
   "metadata": {},
   "source": [
    "Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab3122f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All CSV data extracted and saved to 'combined_data.txt'\n"
     ]
    }
   ],
   "source": [
    "combined_df = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "\n",
    "combined_df.to_csv('combined_data.txt', sep='\\t', index=False)\n",
    "\n",
    "print(\"All CSV data extracted and saved to 'combined_data.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4815c47",
   "metadata": {},
   "source": [
    "Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e1f0325",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e25ddccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scispacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0002b183",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.0/en_core_sci_sm-0.5.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "349087bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.0/en_ner_bc5cdr_md-0.5.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "201de0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "370e9300",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da048cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1700136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d737c3d",
   "metadata": {},
   "source": [
    "Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5780040",
   "metadata": {},
   "source": [
    "Task 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5df5b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 most common words have been saved to 'top_30_words.csv'\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import csv\n",
    "import re\n",
    "\n",
    "with open('combined_data.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "words = re.findall(r'\\b\\w+\\b', text.lower())  \n",
    "\n",
    "word_counts = collections.Counter(words)\n",
    "\n",
    "top_30_words = word_counts.most_common(30)\n",
    "\n",
    "with open('top_30_words.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Word', 'Count'])  \n",
    "    writer.writerows(top_30_words)   \n",
    "\n",
    "print(\"Top 30 most common words have been saved to 'top_30_words.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c76dc5",
   "metadata": {},
   "source": [
    "Task 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef88928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "def get_top_30_tokens(file_path, model_name=\"emilyalsentzer/Bio_ClinicalBERT\", chunk_size=10000, max_seq_length=512):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    token_counts = Counter()\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        while True:\n",
    "            chunk = file.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break \n",
    "            \n",
    "            tokens = tokenizer.tokenize(chunk)\n",
    "\n",
    "            for i in range(0, len(tokens), max_seq_length):\n",
    "                token_chunk = tokens[i:i + max_seq_length]\n",
    "                token_counts.update(token_chunk)\n",
    "\n",
    "    top_30_tokens = token_counts.most_common(30)\n",
    "    \n",
    "    return top_30_tokens\n",
    "\n",
    "top_30_tokens = get_top_30_tokens('combined_data.txt')\n",
    "for token, count in top_30_tokens:\n",
    "    print(f\"Token: {token}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3997cb72",
   "metadata": {},
   "source": [
    "Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72abbcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.0/en_core_sci_sm-0.5.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eb902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.0/en_ner_bc5cdr_md-0.5.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f4c49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from collections import Counter\n",
    "nlp_sci_sm = spacy.load(\"en_core_sci_sm\")\n",
    "nlp_bc5cdr = spacy.load(\"en_ner_bc5cdr_md\")\n",
    "\n",
    "nlp_sci_sm.max_length = 2_000_000 \n",
    "nlp_bc5cdr.max_length = 2_000_000  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09207053",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_sci_sm = spacy.load(\"en_core_sci_sm\")     \n",
    "nlp_bc5cdr = spacy.load(\"en_ner_bc5cdr_md\") \n",
    "\n",
    "biobert_model = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(biobert_model)\n",
    "model = AutoModelForTokenClassification.from_pretrained(biobert_model)\n",
    "biobert_ner = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "with open('combined_data.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "def extract_entities_scispacy(nlp_model, text, chunk_size=1000000):\n",
    "    entities = {\"diseases\": [], \"drugs\": []}\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunk = text[i:i + chunk_size]\n",
    "        doc = nlp_model(chunk)\n",
    "        \n",
    "        entities[\"diseases\"].extend([ent.text for ent in doc.ents if ent.label_ == \"DISEASE\"])\n",
    "        entities[\"drugs\"].extend([ent.text for ent in doc.ents if ent.label_ == \"CHEMICAL\"])\n",
    "    \n",
    "    return entities[\"diseases\"], entities[\"drugs\"]\n",
    "\n",
    "diseases_sci_sm, drugs_sci_sm = extract_entities_scispacy(nlp_sci_sm, text)\n",
    "diseases_bc5cdr, drugs_bc5cdr = extract_entities_scispacy(nlp_bc5cdr, text)\n",
    "\n",
    "def extract_entities_biobert(text):\n",
    "    ner_results = biobert_ner(text)\n",
    "    diseases = [res['word'] for res in ner_results if \"DISEASE\" in res['entity']]\n",
    "    drugs = [res['word'] for res in ner_results if \"CHEMICAL\" in res['entity']]\n",
    "    return diseases, drugs\n",
    "\n",
    "diseases_biobert, drugs_biobert = extract_entities_biobert(text)\n",
    "\n",
    "def count_entities(entities):\n",
    "    return Counter(entities)\n",
    "\n",
    "disease_count_sci_sm = count_entities(diseases_sci_sm)\n",
    "drug_count_sci_sm = count_entities(drugs_sci_sm)\n",
    "\n",
    "disease_count_bc5cdr = count_entities(diseases_bc5cdr)\n",
    "drug_count_bc5cdr = count_entities(drugs_bc5cdr)\n",
    "\n",
    "disease_count_biobert = count_entities(diseases_biobert)\n",
    "drug_count_biobert = count_entities(drugs_biobert)\n",
    "\n",
    "def compare_models(disease_count_model1, drug_count_model1, disease_count_model2, drug_count_model2):\n",
    "    print(\"Total diseases detected (Model 1):\", sum(disease_count_model1.values()))\n",
    "    print(\"Total drugs detected (Model 1):\", sum(drug_count_model1.values()))\n",
    "    \n",
    "    print(\"Total diseases detected (Model 2):\", sum(disease_count_model2.values()))\n",
    "    print(\"Total drugs detected (Model 2):\", sum(drug_count_model2.values()))\n",
    "    \n",
    "    common_diseases = set(disease_count_model1.keys()).intersection(set(disease_count_model2.keys()))\n",
    "    unique_to_model1 = set(disease_count_model1.keys()) - set(disease_count_model2.keys())\n",
    "    unique_to_model2 = set(disease_count_model2.keys()) - set(disease_count_model1.keys())\n",
    "    \n",
    "    print(\"Common diseases:\", len(common_diseases))\n",
    "    print(\"Unique to Model 1:\", len(unique_to_model1))\n",
    "    print(\"Unique to Model 2:\", len(unique_to_model2))\n",
    "    \n",
    "    common_drugs = set(drug_count_model1.keys()).intersection(set(drug_count_model2.keys()))\n",
    "    unique_to_model1_drugs = set(drug_count_model1.keys()) - set(drug_count_model2.keys())\n",
    "    unique_to_model2_drugs = set(drug_count_model2.keys()) - set(drug_count_model1.keys())\n",
    "    \n",
    "    print(\"Common drugs:\", len(common_drugs))\n",
    "    print(\"Unique drugs to Model 1:\", len(unique_to_model1_drugs))\n",
    "    print(\"Unique drugs to Model 2:\", len(unique_to_model2_drugs))\n",
    "\n",
    "print(\"\\nComparing scispaCy (en_core_sci_sm) vs BioBERT\")\n",
    "compare_models(disease_count_sci_sm, drug_count_sci_sm, disease_count_biobert, drug_count_biobert)\n",
    "\n",
    "print(\"\\nComparing scispaCy (en_ner_bc5cdr_md) vs BioBERT\")\n",
    "compare_models(disease_count_bc5cdr, drug_count_bc5cdr, disease_count_biobert, drug_count_biobert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7da6110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
